"""
í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„°ì…‹ë³„ YAML ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ Router í”„ë ˆì„ì›Œí¬ ì‹¤í—˜ ì‹¤í–‰

## í•µì‹¬: íŒ€ì› PCì—ì„œ ë°ì´í„°ì…‹ ê²½ë¡œ ì§€ì • ë°©ë²•
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” `--config` íŒŒì¼ëª…ìœ¼ë¡œ ë°ì´í„°ì…‹ íƒ€ì…ì„ ìë™ ê°ì§€í•œ ë’¤,
ë°ì´í„°ì…‹ì„ **ê¸°ë³¸ ê²½ë¡œ(default)** ì—ì„œ ì½ìŠµë‹ˆë‹¤.
íŒ€ì›ë§ˆë‹¤ ë°ì´í„°ì…‹ ìœ„ì¹˜ê°€ ë‹¤ë¥´ë©´ **`--data_path` í•œ ì¤„ë¡œ ê²½ë¡œë§Œ ë°”ê¿”ì„œ** ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

### 1) ê¸°ë³¸ ê²½ë¡œ(default) ê·œì¹™ (ë ˆí¬ ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ)
- mmlu: `data/mmlu/test/`  (í´ë”, ë‚´ë¶€ì— `*.csv`)
- humaneval: `humaneval_dataset.json` (íŒŒì¼, JSON Lines)
- aqua: `data/aqua/train.json` (íŒŒì¼, JSON)
- gsm8k: `data/gsm8k/test.jsonl` (íŒŒì¼, JSONL)
- math_cot / math_nocot: `data/math/test.jsonl` (íŒŒì¼, JSONL)

### 2) `--data_path`ë¡œ ë®ì–´ì“°ê¸° (íŒ€ì› PC ê²½ë¡œ ì‚¬ìš©)
- mmluëŠ” **í´ë” ê²½ë¡œ**ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤. (ê·¸ í´ë” ì•ˆì— `*.csv` ì¡´ì¬)
  ì˜ˆ) `--data_path "/abs/path/to/mmlu/test"`
- ê·¸ ì™¸(humaneval/aqua/gsm8k/math_*)ëŠ” **íŒŒì¼ ê²½ë¡œ**ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
  ì˜ˆ) `--data_path "/abs/path/to/gsm8k/test.jsonl"`

### 3) ì‹¤í–‰ ì˜ˆì‹œ
    python run_experiment.py --config config/mmlu_config.yaml --num_samples 10
    python run_experiment.py --config config/mmlu_config.yaml --num_samples -1   # ì „ì²´ ë°ì´í„°
    python run_experiment.py --config config/mmlu_config.yaml --data_path "/abs/path/to/mmlu/test" --num_samples 10
    python run_experiment.py --config config/humaneval_config.yaml --data_path "/abs/path/to/humaneval_dataset.json" --num_samples 5
    python run_experiment.py --config config/aqua_config.yaml --data_path "/abs/path/to/aqua/train.json" --num_samples 20
"""

import argparse
import json
import os
import pandas as pd
from typing import Dict, List, Any, Optional
from pathlib import Path
import yaml

from src.router import Router
from src.llm_client import create_llm_client


def detect_dataset_type(config_path: str) -> str:
    """
    Config íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€
    
    Args:
        config_path: Config íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ë°ì´í„°ì…‹ íƒ€ì…: 'mmlu', 'humaneval', 'aqua', 'gsm8k', 'math_cot', 'math_nocot'
    """
    config_name = Path(config_path).stem.lower()
    
    if 'mmlu' in config_name:
        return 'mmlu'
    elif 'humaneval' in config_name:
        return 'humaneval'
    elif 'aqua' in config_name:
        return 'aqua'
    elif 'gsm8k' in config_name or 'gsm' in config_name:
        return 'gsm8k'
    elif 'math_cot' in config_name or 'mathcot' in config_name:
        return 'math_cot'
    elif 'math_nocot' in config_name or 'mathnocot' in config_name:
        return 'math_nocot'
    else:
        # ê¸°ë³¸ê°’: config íŒŒì¼ì—ì„œ ì¶”ë¡  ì‹œë„
        return 'unknown'


def load_mmlu_data(data_path: str = "data/mmlu/test", num_samples: Optional[int] = None) -> List[Dict]:
    """MMLU ë°ì´í„°ì…‹ ë¡œë“œ
    
    MMLU CSV í˜•ì‹: í—¤ë” ì—†ìŒ, question,option1,option2,option3,option4,answer
    """
    import glob
    
    csv_files = glob.glob(f"{data_path}/*.csv")
    if not csv_files:
        raise FileNotFoundError(f"MMLU ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
    
    all_data = []
    for csv_file in csv_files:
        # MMLU CSV í˜•ì‹: í—¤ë” ì—†ìŒ, question,option1,option2,option3,option4,answer
        df = pd.read_csv(csv_file, header=None, names=['question', 'option1', 'option2', 'option3', 'option4', 'answer'])
        
        for _, row in df.iterrows():
            # questionì´ ì œëŒ€ë¡œ ìˆëŠ”ì§€ í™•ì¸
            question = str(row['question']).strip()
            if not question or question.lower() == 'question':
                print(f"âš ï¸  ê²½ê³ : {csv_file}ì˜ í–‰ì—ì„œ questionì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {question}")
                continue
            
            # ì˜µì…˜ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            options = [
                str(row['option1']).strip(),
                str(row['option2']).strip(),
                str(row['option3']).strip(),
                str(row['option4']).strip()
            ]
            
            all_data.append({
                'question': question,
                'options': options,
                'correct': str(row['answer']).strip(),
                'subject': Path(csv_file).stem.replace('_test', '').replace('_val', '')
            })
    
    if num_samples is not None and num_samples > 0:
        all_data = all_data[:num_samples]
    
    return all_data


def load_humaneval_data(data_path: str = "humaneval_dataset.json", num_samples: Optional[int] = None) -> List[Dict]:
    """HumanEval ë°ì´í„°ì…‹ ë¡œë“œ (JSON Lines)"""
    samples = []
    with open(data_path, 'r') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    
    if num_samples is not None and num_samples > 0:
        samples = samples[:num_samples]
    
    return samples


def load_aqua_data(data_path: str = "data/aqua/train.json", num_samples: Optional[int] = None) -> List[Dict]:
    """AQUA ë°ì´í„°ì…‹ ë¡œë“œ (JSON)"""
    with open(data_path, 'r') as f:
        data = json.load(f)
    
    if num_samples is not None and num_samples > 0:
        data = data[:num_samples]
    
    return data


def load_gsm8k_data(data_path: str = "data/gsm8k/test.jsonl", num_samples: Optional[int] = None) -> List[Dict]:
    """GSM8K ë°ì´í„°ì…‹ ë¡œë“œ (JSONL)"""
    samples = []
    with open(data_path, 'r') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    
    if num_samples is not None and num_samples > 0:
        samples = samples[:num_samples]
    
    return samples


def load_math_data(data_path: str = "data/math/test.jsonl", num_samples: Optional[int] = None) -> List[Dict]:
    """Math ë°ì´í„°ì…‹ ë¡œë“œ (JSONL, GSM8Kì™€ ë™ì¼ í˜•ì‹)"""
    return load_gsm8k_data(data_path, num_samples)


def extract_question(sample: Dict, dataset_type: str) -> str:
    """ìƒ˜í”Œì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ"""
    if dataset_type == 'mmlu':
        return sample['question']
    elif dataset_type == 'humaneval':
        return sample['prompt']
    elif dataset_type == 'aqua':
        return sample['question']
    elif dataset_type in ['gsm8k', 'math_cot', 'math_nocot']:
        return sample.get('question', sample.get('problem', ''))
    else:
        # ê¸°ë³¸: 'question' í‚¤ ì‹œë„
        return sample.get('question', str(sample))


def run_experiment(
    config_path: str,
    dataset_type: str,
    num_samples: int = 10,
    data_path: Optional[str] = None,
    use_llm_summary: bool = True,
    save_results: bool = True
) -> Dict[str, Any]:
    """
    Router ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        config_path: Config YAML íŒŒì¼ ê²½ë¡œ
        dataset_type: ë°ì´í„°ì…‹ íƒ€ì…
        num_samples: ì‹¤í—˜í•  ìƒ˜í”Œ ìˆ˜
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        use_llm_summary: LLM summary ì‚¬ìš© ì—¬ë¶€
        save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("="*80)
    print(f"ğŸš€ Router ì‹¤í—˜ ì‹œì‘")
    print("="*80)
    print(f"Config: {config_path}")
    print(f"Dataset: {dataset_type}")
    print(f"Samples: {'ALL' if (num_samples is None or num_samples <= 0) else num_samples}")
    print()
    
    # Router ì´ˆê¸°í™”
    print("ğŸ“¦ Router ì´ˆê¸°í™” ì¤‘...")
    router = Router(config_path)
    print("âœ… Router ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    
    # ê¸°ë³¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •
    if data_path is None:
        if dataset_type == 'mmlu':
            data_path = "data/mmlu/test"
        elif dataset_type == 'humaneval':
            data_path = "humaneval_dataset.json"
        elif dataset_type == 'aqua':
            data_path = "data/aqua/train.json"
        elif dataset_type == 'gsm8k':
            data_path = "data/gsm8k/test.jsonl"
        elif dataset_type in ['math_cot', 'math_nocot']:
            data_path = "data/math/test.jsonl"
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {dataset_type}")

    print(f"  - data_path: {data_path}")
    
    # ë°ì´í„° ë¡œë“œ
    if dataset_type == 'mmlu':
        samples = load_mmlu_data(data_path, num_samples)
    elif dataset_type == 'humaneval':
        samples = load_humaneval_data(data_path, num_samples)
    elif dataset_type == 'aqua':
        samples = load_aqua_data(data_path, num_samples)
    elif dataset_type == 'gsm8k':
        samples = load_gsm8k_data(data_path, num_samples)
    elif dataset_type in ['math_cot', 'math_nocot']:
        samples = load_math_data(data_path, num_samples)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {dataset_type}")
    
    print(f"âœ… {len(samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ\n")
    
    # ì‹¤í—˜ ì‹¤í–‰
    print("ğŸ§ª Router ì‹¤í—˜ ì‹¤í–‰ ì¤‘...\n")
    results = []
    
    for i, sample in enumerate(samples):
        question = extract_question(sample, dataset_type)
        
        # ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not question or question == 'question' or len(str(question).strip()) < 10:
            print(f"  âš ï¸  ê²½ê³ : ìƒ˜í”Œ {i}ì˜ ì§ˆë¬¸ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {question}")
            # ìƒ˜í”Œ ì „ì²´ë¥¼ ì¶œë ¥í•´ì„œ ë””ë²„ê¹…
            print(f"  ìƒ˜í”Œ ë‚´ìš©: {sample}")
            continue
        
        print(f"[{i+1}/{len(samples)}] ì²˜ë¦¬ ì¤‘...")
        print(f"  ì§ˆë¬¸: {question[:100]}..." if len(question) > 100 else f"  ì§ˆë¬¸: {question}")
        
        try:
            result = router.route(question, use_llm_summary=use_llm_summary)
            
            # ìµœì¢… ì—ì´ì „íŠ¸ ì§‘í•©ì„ ëª…í™•í•˜ê²Œ êµ¬ì„±
            final_agents = []
            for role in result['selected_roles']:
                prob = result['role_probabilities'].get(role, 0.0)
                final_agents.append({
                    'agent': role,
                    'probability': float(prob)
                })
            
            # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
            final_agents.sort(key=lambda x: x['probability'], reverse=True)
            
            result_entry = {
                'sample_idx': i,
                'question': str(question)[:500] + "..." if len(str(question)) > 500 else str(question),
                'final_agents': final_agents,  # ìµœì¢… ì—ì´ì „íŠ¸ ì§‘í•© (ëª…í™•í•˜ê²Œ)
                'num_agents': result['num_agents'],
                'selected_blocks': result['selected_blocks'],
                'selected_roles': result['selected_roles'],  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                'num_blocks': result['num_blocks'],
                'block_uncertainty': result['uncertainty']['block_uncertainty'],
                'role_uncertainty': result['uncertainty']['role_uncertainty'],
                'block_probabilities': result['block_probabilities'],
                'role_probabilities': result['role_probabilities']
            }
            
            # ì›ë³¸ ìƒ˜í”Œ ì •ë³´ ì¶”ê°€
            if dataset_type == 'humaneval':
                result_entry['task_id'] = sample.get('task_id', '')
            elif dataset_type == 'aqua':
                result_entry['correct'] = sample.get('correct', '')
            
            results.append(result_entry)
            
            print(f"  âœ… Blocks: {result['num_blocks']}ê°œ")
            print(f"     {result['selected_blocks']}")
            print(f"  ğŸ‘¥ ìµœì¢… ì„ íƒëœ Agents: {result['num_agents']}ê°œ")
            print(f"     {result['selected_roles']}")
            print(f"  ğŸ“Š Uncertainty: Block={result['uncertainty']['block_uncertainty']:.3f}, Role={result['uncertainty']['role_uncertainty']:.3f}")
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            results.append({
                'sample_idx': i,
                'error': str(e)
            })
        
        print()
    
    # í†µê³„ ê³„ì‚°
    successful_results = [r for r in results if 'error' not in r]
    
    if successful_results:
        avg_blocks = sum(r['num_blocks'] for r in successful_results) / len(successful_results)
        avg_agents = sum(r['num_agents'] for r in successful_results) / len(successful_results)
        avg_block_uncertainty = sum(r['block_uncertainty'] for r in successful_results) / len(successful_results)
        avg_role_uncertainty = sum(r['role_uncertainty'] for r in successful_results) / len(successful_results)
        
        summary = {
            'config': config_path,
            'dataset_type': dataset_type,
            'total_samples': len(samples),
            'successful_samples': len(successful_results),
            'failed_samples': len(results) - len(successful_results),
            'avg_blocks': avg_blocks,
            'avg_agents': avg_agents,
            'avg_block_uncertainty': avg_block_uncertainty,
            'avg_role_uncertainty': avg_role_uncertainty,
            'results': results
        }
    else:
        summary = {
            'config': config_path,
            'dataset_type': dataset_type,
            'total_samples': len(samples),
            'successful_samples': 0,
            'failed_samples': len(results),
            'error': 'ëª¨ë“  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨',
            'results': results
        }
    
    # ê²°ê³¼ ì €ì¥
    if save_results:
        os.makedirs('outputs', exist_ok=True)
        output_file = f"outputs/experiment_{dataset_type}_{num_samples}samples.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}\n")
    
    # ìš”ì•½ ì¶œë ¥
    print("="*80)
    print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    if successful_results:
        print(f"âœ… ì„±ê³µ: {len(successful_results)}/{len(samples)}")
        print(f"ğŸ“¦ í‰ê·  Blocks: {avg_blocks:.2f}ê°œ")
        print(f"ğŸ‘¥ í‰ê·  Agents: {avg_agents:.2f}ê°œ")
        print(f"ğŸ“ˆ í‰ê·  Block Uncertainty: {avg_block_uncertainty:.3f}")
        print(f"ğŸ“ˆ í‰ê·  Role Uncertainty: {avg_role_uncertainty:.3f}")
        print()
        print("ğŸ“‹ ìƒ˜í”Œë³„ ì§ˆë¬¸ ë° ìµœì¢… ì„ íƒëœ Agents:")
        for i, result in enumerate(successful_results, 1):
            print(f"\n  [{i}] ì§ˆë¬¸:")
            print(f"      {result['question'][:150]}..." if len(result['question']) > 150 else f"      {result['question']}")
            print(f"      ìµœì¢… ì„ íƒëœ Agents ({result['num_agents']}ê°œ):")
            # final_agentsê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ selected_roles ì‚¬ìš©
            if 'final_agents' in result:
                for agent_info in result['final_agents']:
                    print(f"        - {agent_info['agent']} (í™•ë¥ : {agent_info['probability']:.3f})")
            else:
                for role in result['selected_roles']:
                    prob = result['role_probabilities'].get(role, 0.0)
                    print(f"        - {role} (í™•ë¥ : {prob:.3f})")
    else:
        print("âŒ ëª¨ë“  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨")
    print("="*80)
    
    return summary


def main():
    parser = argparse.ArgumentParser(description='Router í”„ë ˆì„ì›Œí¬ í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--config', type=str, required=True,
                        help='Config YAML íŒŒì¼ ê²½ë¡œ (ì˜ˆ: config/mmlu_config.yaml)')
    parser.add_argument('--num_samples', type=int, default=-1,
                        help='ì‹¤í—˜í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: -1, ì¦‰ ì „ì²´). íŠ¹ì • ê°œìˆ˜ë§Œ ëŒë¦¬ë ¤ë©´ ì–‘ìˆ˜ë¥¼ ë„£ìœ¼ì„¸ìš” (ì˜ˆ: 10).')
    parser.add_argument('--data_path', type=str, default=None,
                        help=(
                            "ë°ì´í„° ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•´ ê¸°ë³¸ ê²½ë¡œë¥¼ ë®ì–´ì”ë‹ˆë‹¤. "
                            "mmluëŠ” í´ë” ê²½ë¡œ(ë‚´ë¶€ì— *.csv), ê·¸ ì™¸ëŠ” íŒŒì¼ ê²½ë¡œë¥¼ ë„£ìœ¼ì„¸ìš”. "
                            "ì˜ˆ) --data_path data/mmlu/test  ë˜ëŠ”  --data_path /abs/path/to/gsm8k/test.jsonl"
                        ))
    parser.add_argument('--no_llm', action='store_true',
                        help='LLM summary ì‚¬ìš© ì•ˆ í•¨ (embeddingë§Œ ì‚¬ìš©)')
    parser.add_argument('--dataset_type', type=str, default=None,
                        help='ë°ì´í„°ì…‹ íƒ€ì… ê°•ì œ ì§€ì • (mmlu, humaneval, aqua, gsm8k, math_cot, math_nocot)')
    parser.add_argument('--no_save', action='store_true',
                        help='ê²°ê³¼ ì €ì¥ ì•ˆ í•¨')
    
    args = parser.parse_args()
    
    # ë°ì´í„°ì…‹ íƒ€ì… ê°ì§€
    if args.dataset_type:
        dataset_type = args.dataset_type
    else:
        dataset_type = detect_dataset_type(args.config)
        if dataset_type == 'unknown':
            print("âš ï¸  ë°ì´í„°ì…‹ íƒ€ì…ì„ ìë™ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --dataset_type ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return
    
    # ì‹¤í—˜ ì‹¤í–‰
    summary = run_experiment(
        config_path=args.config,
        dataset_type=dataset_type,
        num_samples=args.num_samples,
        data_path=args.data_path,
        use_llm_summary=not args.no_llm,
        save_results=not args.no_save
    )
    
    print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
